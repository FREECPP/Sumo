{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of sensor data at A034"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this file is to evaluate real sensor data from junction A034 in Darmstadt. The city of Darmstadt employs various technologies to monitor traffic at different locations throughout the city. Similarly, the simulation software SUMO provides the capability to simulate sensor data.\n",
    "\n",
    "This feature is particularly useful when aiming to align simulated traffic with real-world data. By comparing the data from simulated sensors with the actual sensor readings, adjustments can be made to the simulation until the traffic patterns closely match the real-world measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approach to achieving the purpose of this file begins by extracting all the necessary data into a DataFrame. Upon reviewing the raw data, it becomes clear that not all the information in the CSV file is required. Therefore, the first step is to filter out only the relevant data from the CSV file or trim the DataFrame until only the essential information remains.\n",
    "\n",
    "Next, a function is implemented to aggregate the number of vehicles detected by the sensor based on a specified time interval. Finally, the results are not only presented as a DataFrame but also visualized in forms such as line graphs or heatmaps for better interpretation and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alle notewendigen Bibliotheken einbinden\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from typing import List\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunatly the provided csv-files are not completely consistant, sometimes some rows look like this: \n",
    "\n",
    ">A034;18.09.2023 09:57:00;18.09.2023 11:57:00;60000;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n",
    ">\n",
    ">A034;18.09.2023 09:58:00;18.09.2023 11:58:00;60000;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n",
    ">\n",
    ">A034;18.09.2023 09:59:00;18.09.2023 11:59:00;60000;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n",
    "\n",
    "In this case, the sensors that should provide data between the semicolons either malfunctioned or another issue caused the absence of data. Ideally, we would assume that these missing values should be recorded as 0, since the sensors did not provide any data. Additionally, our function cannot handle **\"not a number\" (NaN)** inputs properly. Therefore, we require another function to fill the empty spaces between the semicolons with 0. The function **zero_between_semicolons** is designed specifically to address this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_between_semikolons(inputfile, outputfile,replace,counter):\n",
    "    \n",
    "    # Datei einlesen\n",
    "    with open(inputfile, \"r\") as file:\n",
    "        data = file.read()\n",
    "\n",
    "# Ersetzen von \";;\" durch \";0;\"\n",
    "    data_modified = re.sub(r\";;\", replace, data)\n",
    "    data_modified = re.sub(r\";\\n\",\"\\n\",data_modified)\n",
    "\n",
    "# In eine neue Datei schreiben\n",
    "    with open(outputfile, \"w\") as file:\n",
    "        file.write(data_modified)\n",
    "    while(counter < 2):\n",
    "        counter = counter +1\n",
    "        zero_between_semikolons(outputfile,outputfile,replace,counter)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hier nutzen wir die Tatsache das der Name des Detektors immer am Anfang der Datei steht\n",
    "def get_detector_name(file_name):\n",
    "    posOfUnderscore = file_name.find(\"_\")\n",
    "    return file_name[1:posOfUnderscore]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function create_data_analysis is designed to process, analyze, and visualize traffic data collected by multiple sensors. This data is typically stored in a CSV file with raw readings from sensors, which may include missing values or formatting issues. The function handles these challenges, aggregates the data into defined time intervals, and provides insights through visualizations.\n",
    "Key Features:\n",
    "\n",
    "1. Data Cleaning:\n",
    "- Handles missing values in the CSV file by replacing empty or invalid entries with zero using a helper function zero_between_semicolons.\n",
    "- Renames columns for consistency and ease of analysis.\n",
    "\n",
    "2. Time-Based Aggregation:\n",
    "- Groups sensor data into user-defined time intervals (e.g., 5, 10, or 30 minutes) to better analyze trends over time.\n",
    "- Uses floor operations to round timestamps to the nearest interval.\n",
    "\n",
    "3. Data Transformation:\n",
    "- Converts the aggregated data from a \"wide\" format (with sensors as columns) to a \"long\" format, making it more suitable for advanced visualizations and analysis.\n",
    "- Adds placeholder columns for various vehicle types and speeds, enabling further extension or integration with additional datasets.\n",
    "\n",
    "4. Visualization:\n",
    "- Generates a heatmap to display traffic intensity (qPKW) for each sensor over time, highlighting patterns and anomalies.\n",
    "- Creates a scatterplot and line plot to visualize average traffic values (qPKW) across sensors, enabling comparisons between sensor performance.\n",
    "\n",
    "5. Output:\n",
    "- Saves the cleaned and processed data as a tab-separated file for further use.\n",
    "- Provides interactive visual insights to assist in traffic analysis and simulation adjustments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_analysis(csv_files: List[str], output_file, intervall_in_min):\n",
    "    #Zeitintervall vorbereiten für dt.floor vorbereiten\n",
    "    time_interval = str(intervall_in_min) + \"T\"\n",
    "    #Erstellen des Spaltennamens fürs Zeitintervall\n",
    "    time_colum_name = \"Time Intervall : \" + str(intervall_in_min) + \" min\"\n",
    "\n",
    "    #CSV-Datei vorbereiten falls NAN oder semikolons am ende Problematik von A034_20230401_000000_-_20230411_100000_1min.csv\n",
    "    corrected_csv_files = []\n",
    "    for i in csv_files:\n",
    "        print(\"First\" + i)\n",
    "        corrected_csv_file = \"C\" + i\n",
    "        zero_between_semikolons(i, corrected_csv_file,\";0;\",0)\n",
    "        corrected_csv_files.append(corrected_csv_file)\n",
    "\n",
    "    #Spaltennamen erhalten\n",
    "    for file in range(len(corrected_csv_files)):\n",
    "        if file == 0: #initiale erstellung des ersten dataFrames, alle nachfolgenden sollen nur noch angeängt werden\n",
    "            print(corrected_csv_files[file])\n",
    "            A034 = pd.read_csv(corrected_csv_files[file],sep=\";\")\n",
    "            spalten = A034.columns #Columns gibt die Spaltennamen von A034 zurück\n",
    "            #Aus Spalten werden nur die spalten herausgesucht, welche \"(Belegungen/Intervall)\" im Namen haben\n",
    "            belegungen_columns = [col for col in spalten if '(Belegungen/Intervall)' in col]\n",
    "            #Falls es Spalten gibt mit dem Namen \"Intervallbeginn (Lokalzeit)\", werden diese an den Anfang von belegungen_columns gestellt\n",
    "            if \"Intervallbeginn (Lokalzeit)\" in spalten:\n",
    "                belegungen_columns.insert(0, \"Intervallbeginn (Lokalzeit)\")\n",
    "\n",
    "            #Detectorname herausfinden anhand Dateiname\n",
    "            detector_name = get_detector_name(corrected_csv_files[file])\n",
    "            \n",
    "            # Automatische Umwandlung\n",
    "            converted_columns = [f\"{detector_name}_{col.split(' ')[0]}\" for col in belegungen_columns] #sorgt dafür das aus dem spaltennamen: Sensor1 (Belegung/Intervall) der Spaltenname A034_Sensor1 wird\n",
    "            \n",
    "            for i in range(len(belegungen_columns)):\n",
    "                A034 = A034.rename(columns={belegungen_columns[i]:converted_columns[i]}) #benennt die spalten belegungen_columns entsprechend converted_columns\n",
    "            A034[converted_columns[0]] = pd.to_datetime(A034[converted_columns[0]],format=\"%d.%m.%Y %H:%M:%S\") # convertiert die erste spalte in das format datetime\n",
    "            A034[time_colum_name] = A034[converted_columns[0]].dt.floor(time_interval)  # Zeit auf 5-Minuten-Intervalle runden\n",
    "        \n",
    "            result = A034.groupby(time_colum_name)[converted_columns[1:]].sum().reset_index() #gruppiert alle gleichen inhalte der spalte \"time_colum_name\" und summiert die inhalte der übrigen spalten auf\n",
    "\n",
    "            start_time = result[time_colum_name].min() # speichert den kleinsten wert der Spalte time_colum_name als start_time\n",
    "            result['Time'] = (result[time_colum_name] - start_time).dt.total_seconds() / 60 # zieht für jeden Wert die start_time ab und berechnet so die Minuten ausgehend vom begin\n",
    "            result = result.drop(columns=[time_colum_name], errors=\"ignore\") \n",
    "            \n",
    "                \n",
    "            #Erreicht die gewünschte Ausgabe\n",
    "            #um von wide nach long zu transformieren (alle spalten außer time zu zeilen wandeln\n",
    "            melted = pd.melt(\n",
    "                result, \n",
    "                #id_vars=[time_colum_name, 'Time'],  # Version mit Globaler zeit \n",
    "                id_vars=['Time'],  # Zeit bleibt als Spalte # Version ohne Globale Zeit\n",
    "                var_name=\"Detector\", # Name für die neuen Zeilen\n",
    "                value_name=\"qPKW\",         # Name der Werte\n",
    "\n",
    "            )\n",
    "            melted.set_index(['Time', \"Detector\"], inplace=True)\n",
    "            melted.sort_index(inplace=True)\n",
    "            melted[\"qFG\"] = 0\n",
    "            melted[\"qRF\"] = 0\n",
    "            melted[\"qBus\"] = 0\n",
    "            melted[\"qLKW\"] = 0\n",
    "            melted[\"vPKW\"] = 0\n",
    "            melted[\"vFG\"] = 0\n",
    "            melted[\"vRF\"] = 0\n",
    "            melted[\"vBus\"] = 0\n",
    "            melted[\"vLKW\"] = 0\n",
    "            \n",
    "            melted = melted.swaplevel(\"Detector\",'Time')\n",
    "            melted = melted.sort_index(level=['Time', 'Detector'], ascending=[True, True])\n",
    "            melted = melted.reset_index()\n",
    "            #in melted befindet sich nun der Datensatz, dieser wird überschrieben wenn der nächste Input eingelesen wird\n",
    "        else: \n",
    "            following_df = pd.read_csv(corrected_csv_files[file],sep=\";\")\n",
    "            spalten = following_df.columns #Columns gibt die Spaltennamen von A034 zurück\n",
    "            #Aus Spalten werden nur die spalten herausgesucht, welche \"(Belegungen/Intervall)\" im Namen haben\n",
    "            belegungen_columns = [col for col in spalten if '(Belegungen/Intervall)' in col]\n",
    "            #Falls es Spalten gibt mit dem Namen \"Intervallbeginn (Lokalzeit)\", werden diese an den Anfang von belegungen_columns gestellt\n",
    "            if \"Intervallbeginn (Lokalzeit)\" in spalten:\n",
    "                belegungen_columns.insert(0, \"Intervallbeginn (Lokalzeit)\")\n",
    "\n",
    "            #Detectorname herausfinden anhand Dateiname\n",
    "            detector_name = get_detector_name(corrected_csv_files[file])\n",
    "            \n",
    "            # Automatische Umwandlung\n",
    "            converted_columns = [f\"{detector_name}_{col.split(' ')[0]}\" for col in belegungen_columns] #sorgt dafür das aus dem spaltennamen: Sensor1 (Belegung/Intervall) der Spaltenname A034_Sensor1 wird\n",
    "            \n",
    "            for i in range(len(belegungen_columns)):\n",
    "                following_df = following_df.rename(columns={belegungen_columns[i]:converted_columns[i]}) #benennt die spalten belegungen_columns entsprechend converted_columns\n",
    "            following_df[converted_columns[0]] = pd.to_datetime(following_df[converted_columns[0]],format=\"%d.%m.%Y %H:%M:%S\") # convertiert die erste spalte in das format datetime\n",
    "            following_df[time_colum_name] = following_df[converted_columns[0]].dt.floor(time_interval)  # Zeit auf 5-Minuten-Intervalle runden\n",
    "        \n",
    "            result_following_df = following_df.groupby(time_colum_name)[converted_columns[1:]].sum().reset_index() #gruppiert alle gleichen inhalte der spalte \"time_colum_name\" und summiert die inhalte der übrigen spalten auf\n",
    "\n",
    "            start_time = result_following_df[time_colum_name].min() # speichert den kleinsten wert der Spalte time_colum_name als start_time\n",
    "            result_following_df['Time'] = (result_following_df[time_colum_name] - start_time).dt.total_seconds() / 60 # zieht für jeden Wert die start_time ab und berechnet so die Minuten ausgehend vom begin\n",
    "            result_following_df = result_following_df.drop(columns=[time_colum_name], errors=\"ignore\") \n",
    "            \n",
    "                \n",
    "            #Erreicht die gewünschte Ausgabe\n",
    "            #um von wide nach long zu transformieren (alle spalten außer time zu zeilen wandeln\n",
    "            melted_following_df = pd.melt(\n",
    "                result_following_df, \n",
    "                #id_vars=[time_colum_name, 'Time'],  # Version mit Globaler zeit \n",
    "                id_vars=['Time'],  # Zeit bleibt als Spalte # Version ohne Globale Zeit\n",
    "                var_name=\"Detector\", # Name für die neuen Zeilen\n",
    "                value_name=\"qPKW\",         # Name der Werte\n",
    "\n",
    "            )\n",
    "            melted_following_df.set_index(['Time', \"Detector\"], inplace=True)\n",
    "            melted_following_df.sort_index(inplace=True)\n",
    "            melted_following_df[\"qFG\"] = 0\n",
    "            melted_following_df[\"qRF\"] = 0\n",
    "            melted_following_df[\"qBus\"] = 0\n",
    "            melted_following_df[\"qLKW\"] = 0\n",
    "            melted_following_df[\"vPKW\"] = 0\n",
    "            melted_following_df[\"vFG\"] = 0\n",
    "            melted_following_df[\"vRF\"] = 0\n",
    "            melted_following_df[\"vBus\"] = 0\n",
    "            melted_following_df[\"vLKW\"] = 0\n",
    "            \n",
    "            melted_following_df = melted_following_df.swaplevel(\"Detector\",'Time')\n",
    "            melted_following_df = melted_following_df.sort_index(level=['Time', 'Detector'], ascending=[True, True])\n",
    "            melted_following_df = melted_following_df.reset_index()\n",
    "            #in melted befindet sich nun der Datensatz, dieser wird überschrieben wenn der nächste Input eingelesen wird \n",
    "        melted = pd.concat([melted, melted_following_df], ignore_index=True)\n",
    "        #Daten in txt-Datei speichern\n",
    "        #melted.to_csv(output_file, sep=\"\\t\", index=False)\n",
    "        #melted.to_csv(\"SemikolonSepperated_\" + output_file , sep=\";\",index = False, header= False )\n",
    "    melted.to_csv(\"SemikolonSepperatedTest_\" + output_file , sep=\";\",index = False)\n",
    "    return melted\n",
    "        # Learnings: \n",
    "        # Relevant waren das der Header \"Detector\", \"Time\", und \"qPKW\" enthält und das die Datei nicht durch\n",
    "        # Tabulatoren sondern durch Semikolons getrennt wurde\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To-Do:\n",
    "- mehrere Files übergeben \n",
    "- erstes File gibt header an\n",
    "- restlichen Files werden entsprechend der Zeit und dem Sensornamen einsortiert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FirstA034_20230101_000000_-_20230201_000000_1min.csv\n",
      "FirstA036_20230101_000000_-_20230201_000000_1min.csv\n",
      "CA034_20230101_000000_-_20230201_000000_1min.csv\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'melted_following_df' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/opt/homebrew/Cellar/sumo/1.20.0/share/sumo/tools/bring_real_wrld_data_in_right_format/A034_analysis_relative_time.ipynb Zelle 20\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/opt/homebrew/Cellar/sumo/1.20.0/share/sumo/tools/bring_real_wrld_data_in_right_format/A034_analysis_relative_time.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m files \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mA034_20230101_000000_-_20230201_000000_1min.csv\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mA036_20230101_000000_-_20230201_000000_1min.csv\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/opt/homebrew/Cellar/sumo/1.20.0/share/sumo/tools/bring_real_wrld_data_in_right_format/A034_analysis_relative_time.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m create_data_analysis(files,\u001b[39m\"\u001b[39;49m\u001b[39moutput3.txt\u001b[39;49m\u001b[39m\"\u001b[39;49m,\u001b[39m10\u001b[39;49m)\n",
      "\u001b[1;32m/opt/homebrew/Cellar/sumo/1.20.0/share/sumo/tools/bring_real_wrld_data_in_right_format/A034_analysis_relative_time.ipynb Zelle 20\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/opt/homebrew/Cellar/sumo/1.20.0/share/sumo/tools/bring_real_wrld_data_in_right_format/A034_analysis_relative_time.ipynb#X23sZmlsZQ%3D%3D?line=121'>122</a>\u001b[0m         melted_following_df \u001b[39m=\u001b[39m melted_following_df\u001b[39m.\u001b[39mreset_index()\n\u001b[1;32m    <a href='vscode-notebook-cell:/opt/homebrew/Cellar/sumo/1.20.0/share/sumo/tools/bring_real_wrld_data_in_right_format/A034_analysis_relative_time.ipynb#X23sZmlsZQ%3D%3D?line=122'>123</a>\u001b[0m         \u001b[39m#in melted befindet sich nun der Datensatz, dieser wird überschrieben wenn der nächste Input eingelesen wird \u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/opt/homebrew/Cellar/sumo/1.20.0/share/sumo/tools/bring_real_wrld_data_in_right_format/A034_analysis_relative_time.ipynb#X23sZmlsZQ%3D%3D?line=123'>124</a>\u001b[0m     melted \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([melted, melted_following_df], ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/opt/homebrew/Cellar/sumo/1.20.0/share/sumo/tools/bring_real_wrld_data_in_right_format/A034_analysis_relative_time.ipynb#X23sZmlsZQ%3D%3D?line=124'>125</a>\u001b[0m     \u001b[39m#Daten in txt-Datei speichern\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/opt/homebrew/Cellar/sumo/1.20.0/share/sumo/tools/bring_real_wrld_data_in_right_format/A034_analysis_relative_time.ipynb#X23sZmlsZQ%3D%3D?line=125'>126</a>\u001b[0m     \u001b[39m#melted.to_csv(output_file, sep=\"\\t\", index=False)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/opt/homebrew/Cellar/sumo/1.20.0/share/sumo/tools/bring_real_wrld_data_in_right_format/A034_analysis_relative_time.ipynb#X23sZmlsZQ%3D%3D?line=126'>127</a>\u001b[0m     \u001b[39m#melted.to_csv(\"SemikolonSepperated_\" + output_file , sep=\";\",index = False, header= False )\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/opt/homebrew/Cellar/sumo/1.20.0/share/sumo/tools/bring_real_wrld_data_in_right_format/A034_analysis_relative_time.ipynb#X23sZmlsZQ%3D%3D?line=127'>128</a>\u001b[0m melted\u001b[39m.\u001b[39mto_csv(\u001b[39m\"\u001b[39m\u001b[39mSemikolonSepperatedTest_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m output_file , sep\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m;\u001b[39m\u001b[39m\"\u001b[39m,index \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'melted_following_df' referenced before assignment"
     ]
    }
   ],
   "source": [
    "files = [\"A034_20230101_000000_-_20230201_000000_1min.csv\", \"A036_20230101_000000_-_20230201_000000_1min.csv\"]\n",
    "create_data_analysis(files,\"output3.txt\",10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WF-Datenanalyse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
